# Normalization

Computers are very fast at determining whether two strings are identical, which is useful not only for collation, but also for counting word frequencies and similar operations. What’s more challenging is that there may be variant ways of writing what a human considers (and wants to treat as) the same thing, but because they aren’t string-equal, a computer cannot quickly recognize that they should be regarded as the same. Normalization is the process of telling the computer that strings that are different should be regarded as the same. 

Normalization is important in the analysis of variation in critical editing. Editors of critical editions typically include only “significant” variants in a critical apparatus, where the definition of “significant” may vary according to the language, the texts, and the research goals. Normalization removes non-significant variation and retains significant variation, so that it becomes possible to say which textual witnesses agree or disagree with one another with respect to significant variation, without contamination from insignificant variation.

A simple example of where normalization is useful outside collation is that if you want to know how often a particular word occurs in your English-language text, you probably don’t care whether it’s capitalized (at the beginning of a sentence) or not. To ignore case differences, you might implement a normalization operation that converts your tokens to lower case, so that you can count forms with and without initial capitalization together. When dealing with differences in capitalization, the normalization could be performed either before or after tokenization, by either lower-casing the entire document before dividing it into words or lower-casing the individual tokens after tokenization. Some types of normalization, though, are most easily performed after tokenization, which is why GM orders normalization after tokenization.

Normalization may be performed on the text being processed or on a shadow copy of the text. If you are counting word frequencies, you might just lowercase the entire text, neutralizing irretrievably the original difference between word tokens with and without initial capitalization. But for the parallel alignment of variants in a critical edition, you might want to retain the original capitalization patterns in your output, while ignoring capitalization for alignment purposes, that is, for determining where two witnesses have what a human would consider “the same reading”. In this case you can create normalized shadow copies, or surrogates, or the tokens, compare the shadow copies to determine where there is and is not significant variation, but output the original, non-normalized forms for eventual visualization.

### White-space normalization

In normal English writing, white space separates words, but much of the time we don’t care about the specific type of white space. For example, by default, [CollateX](https://pypi.python.org/pypi/collatex) ignores trailing whitespace for alignment purposes, so that, for example, it will treat “koala” (five characters) as identical to “koala ” (six characters, including a trailing space).

While we might describe this casually by saying that CollateX ignores trailing whitespace during alignment, a more accurate description is that the special treatment of trailing whitespace is the default normalization (Gothenburg stage 2) built into CollateX, that it happens before alignment (stage 3), and that alignment then operates on the normalized forms. For this reason, when we tokenize “Faith, Hope, and Charity” and “Faith, Hope and Charity”, with and without the serial comma, in CollateX using the default (built-in) tokenizer, the first sentence has a token “Hope” (four characters), followed by a one-character token that consists of just a comma, and the second has a five-character token “Hope&nbsp;” (with a trailing space). CollateX will regard both as containing a four-character token “Hope” because by default it uses a normalized shadow copy that ignores (normalizes away) trailing white space. 

Similarly, in a plain (without markup) poetic text, we might care about the difference between space characters and new-line characters that difference is useful for identifying end rhyme. But in a plain prose text, line ends may be an artifact of the size of the printed page, and we might choose to treat words separated by new-line characters identically to words separated by space characters.

### Unicode normalization

[Unicode](http://www.unicode.org) is the system for representing the characters of writing systems in a computer. Representing all characters of all writing systems is a goal that Unicode approaches, but perhaps will never wholly achieve. See, for example, [“I can text you a pile of poo, but I can’t write my name”](https://modelviewculture.com/pieces/i-can-text-you-a-pile-of-poo-but-i-cant-write-my-name). Unicode is built into all major modern operating systems and applications, and for the most part it just works, and you don’t need to think about it. 

In some situations, though, it is possible to represent the same character (unit of writing) in more than one way. For example, “á” might be encoded as either a single a-with-acute-accent character, or as a sequence of two characters: a regular “a” and a floating acute diacritic that is rendered over it. These representations look the same and are regarded as the same by a human, but because they are not string-equal, a computer that performs just raw string comparison doesn't know that they are informationally identical. Unicode incorporates a process called [Unicode normalization](https://en.wikipedia.org/wiki/Unicode_equivalence), which coverts such variants to a consistent representation. If you don’t care about this type of variation, you’ll probably want to normalize it away.

**RESUME HERE**
