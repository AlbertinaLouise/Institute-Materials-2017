{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Python Clinic Day 2: Corpus Processing\n",
    "\n",
    "Na-Rae Han (naraehan@pitt.edu), 07/13/2017, [Pittsburgh NEH Institute \"Make Your Edition\"](https://github.com/Pittsburgh-NEH-Institute/Institute-Materials-2017) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "- This tutorial is found on https://github.com/Pittsburgh-NEH-Institute/Institute-Materials-2017/tree/master/schedule/week_1\n",
    "- Download and unzip the \"C-Span Inaugural Address Corpus\", available on NLTK's corpora page: http://www.nltk.org/nltk_data/\n",
    "- Place the unzipped \"inaugural\" folder **on your DESKTOP** \n",
    "\n",
    "Jupyter tips:\n",
    "- Shift+ENTER to run cell, go to next cell\n",
    "- Alt+ENTER to run cell, create a new cell below\n",
    "\n",
    "More on https://www.cheatography.com/weidadeyue/cheat-sheets/jupyter-notebook/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review \n",
    "- Let's review [what we learned yesterday](Python Clinic Day 1.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing a single text file, continued\n",
    "### Reading in a text file\n",
    "* Start with opening up the 1789 Washington address, using `open(filename).read()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = 'C:/Users/narae/Desktop/inaugural/1789-Washington.txt'  # Mac users should leave out C:\n",
    "wtxt = open(myfile).read()\n",
    "print(wtxt[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text, compile frequency count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk    # Don't forget to import nltk\n",
    "%pprint    # Turn off/on pretty printing (prints too many lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtokens = nltk.word_tokenize(wtxt)\n",
    "len(wtokens)     # Number of words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary of frequency count\n",
    "wfreq = nltk.FreqDist(wtokens)\n",
    "wfreq['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wfreq)      # Number of unique words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wfreq.most_common(40)     # 40 most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average sentence length, frequency of long words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentcount = wfreq['.'] + wfreq['?'] + wfreq['!']  # Assuming every sentence ends with ., ! or \n",
    "print(sentcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens include symbols and punctuation. First 50 tokens:\n",
    "wtokens[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtokens_nosym = [t for t in wtokens if t.isalnum()]    # alpha-numeric tokens only\n",
    "len(wtokens_nosym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try \"n't\", \"20th\", \".\"\n",
    "\"n't\".isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 50 tokens, alpha-numeric tokens only: \n",
    "wtokens_nosym[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wtokens_nosym)/sentcount     # Average sentence length in number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wfreq if len(w) >= 13]       # all 13+ character words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long = [w for w in wfreq if len(w) >= 13] \n",
    "for w in long :\n",
    "    print(w, len(w), wfreq[w])               # long words tend to be less frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing a  corpus\n",
    "\n",
    "- NLTK can read in an entire corpus from a directory (the 'root' directory).\n",
    "- As it reads in a corpus, it applies word tokenization (shown below) and sentence tokenization (not shown here). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = 'C:/Users/narae/Desktop/inaugural'  # Mac users should leave out C:\n",
    "inaug = PlaintextCorpusReader(corpus_root, '.*txt')  # all files ending in 'txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .txt file names as file IDs\n",
    "inaug.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK automatically tokenizes the corpus. First 50 words: \n",
    "print(inaug.words()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also specify individual file ID. First 50 words from Obama 2009:\n",
    "print(inaug.words('2009-Obama.txt')[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK automatically segments sentences too, which are accessed through .sents()\n",
    "print(inaug.sents('2009-Obama.txt')[0])   # first sentence\n",
    "print(inaug.sents('2009-Obama.txt')[1])   # 2nd sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long are these speeches in terms of word and sentence count?\n",
    "print('Washington 1789:', len(inaug.words('1789-Washington.txt')), len(inaug.sents('1789-Washington.txt')))\n",
    "print('Obama 2009:', len(inaug.words('2009-Obama.txt')), len(inaug.sents('2009-Obama.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for-loop through file IDs and print out word count. \n",
    "for f in inaug.fileids():\n",
    "    print(len(inaug.words(f)), f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouble shooting \n",
    "- Unfortunately, 2005 Bush file produces a Unicode encoding error. \n",
    "- Let's make a new text file from [http://www.presidency.ucsb.edu/inaugurals.php](http://www.presidency.ucsb.edu/inaugurals.php)\n",
    "- Copy text and paste in Notepad (Windows) or T. Make sure to choose UTF-8 encoding and not ANSI. \n",
    "- The text files are locked; We will need to save, halt and then re-start the Python notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus size in number of words\n",
    "print(len(inaug.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building word frequency distribution for the entire corpus\n",
    "inaug_freq = nltk.FreqDist(inaug.words())\n",
    "inaug_freq.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What next?\n",
    "Take a Python course. There are many online courses available on Coursera, Udemy, EdX, and more. \n",
    "\n",
    " [Coursera](http://www.coursera.org), [EdX](https://www.edx.org/), [udemy](https://www.udemy.com/courses/), [DataCamp](https://www.datacamp.com/courses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
